# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TQMQioHnzjAjVOdvWJTTSPVMY3UM5rjY
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from keras.preprocessing.text import Tokenizer
from tensorflow import keras
import tensorflow as tf

dataTrain = pd.read_csv('finalcleandata.csv')

tokenizer = Tokenizer(num_words=10000, lower=True)

dataTrain = dataTrain['message']

xtrain = tokenizer.fit_on_texts(dataTrain)
xfinal = tokenizer.texts_to_sequences(dataTrain)

import numpy as np

dictionary = dict()
data = open("/content/drive/My Drive/glove.twitter.27B.200d.txt",encoding='utf8')
for line in data:
    values = line.split()
    word = values[0]
    try:
        coefs = np.asarray(values[1:],dtype='float32')
    except Exception as e:
        pass
    dictionary[word] = coefs
data.close()

dictionary = {k: (v+3) for (k, v) in dictionary.items()}
dictionary["<PAD>"] = 0  # actual zero - means they are like multiplied by 0
dictionary["b'"] = 1
dictionary["<UNK>"] = 2
dictionary["<UNUSED>"] = 3

PAD = 0  # a hyperparameter
xtrain_modified = keras.preprocessing.sequence.pad_sequences(
    xfinal, value=PAD, padding='post', maxlen=64)

dataTrain = pd.read_csv('finalcleandata.csv')
# ytrain1 = dataTrain[['toxic','severe_toxic','obscene','threat','insult','identity_hate']] 
ytrain1 = dataTrain['toxic']
ytrain2 = dataTrain['obscene']
ytrain3 = dataTrain['insult']
ytrain4 = dataTrain['racism']
ytrain5 = dataTrain['sexism']

xt = xtrain_modified[:10000] 
xv = xtrain_modified[10000:12500] 
xtt = xtrain_modified[12500:] 

yt1 = ytrain1[:10000]
yv1 = ytrain1[10000:12500] 
ytt1 = ytrain1[12500:] 


yt2 = ytrain2[:10000]
yv2 = ytrain2[10000:12500] 
ytt2 = ytrain2[12500:] 

yt3 = ytrain3[:10000]
yv3 = ytrain3[10000:12500] 
ytt3 = ytrain3[12500:] 

yt4 = ytrain4[:10000]
yv4 = ytrain4[10000:12500]
ytt4 = ytrain4[12500:] 


yt5 = ytrain5[:10000]
yv5 = ytrain5[10000:12500] 
ytt5 = ytrain5[12500:]

def model_hate_64():
  model = keras.Sequential()
  model.add(keras.layers.Embedding(10000, 200))
  model.add(keras.layers.GlobalAveragePooling1D())
  model.add(keras.layers.Dense(64, activation=tf.nn.relu))
  model.add(keras.layers.Dense(64, activation=tf.nn.relu))
  model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))
  return model

def model_hate_128():
  model = keras.Sequential()
  model.add(keras.layers.Embedding(10000, 200))
  model.add(keras.layers.GlobalAveragePooling1D())
  model.add(keras.layers.Dense(128, activation=tf.nn.relu))
  model.add(keras.layers.Dense(128, activation=tf.nn.relu))
  model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))
  return model

model1=model_hate_128()
model2=model_hate_128()
model3=model_hate_128()
model4=model_hate_64()
model5=model_hate_64()

model1.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
model2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
model3.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
model4.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
model5.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])


# print(model.summary())

print("TOXIC")
history1 = model1.fit(xt,yt1,epochs=5,batch_size=64,validation_data=(xv,yv1))
print("OBSCENE")
history2 = model2.fit(xt,yt2,epochs=5,batch_size=64,validation_data=(xv,yv2))
print("INSULT")
history3 = model3.fit(xt,yt3,epochs=5,batch_size=64,validation_data=(xv,yv3))
print("RACISM")
history4 = model4.fit(xt,yt4,epochs=5,batch_size=64,validation_data=(xv,yv4))
print("SEXISM")
history5 = model5.fit(xt,yt5,epochs=5,batch_size=64,validation_data=(xv,yv5))

from tensorflow.keras.utils import plot_model
plot_model(model1,to_file='toxic.png',show_shapes=True,show_layer_names=True)